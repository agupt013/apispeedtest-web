{
  "generated_at": "2025-08-30T04:20:50.631540+00:00",
  "runs": 3,
  "mode": "both",
  "models": [
    "openai:gpt-4o-mini",
    "openai:gpt-4o",
    "openai:gpt-4.1",
    "openai:gpt-3.5-turbo",
    "openai:gpt-3.5-turbo-16k",
    "openai:gpt-4.1-mini",
    "openai:gpt-4-turbo",
    "openai:gpt-4",
    "openai:o3",
    "openai:o3-mini",
    "openai:o4-mini",
    "openai:gpt-4o-2024-08-06",
    "openai:gpt-5",
    "azure:gpt-4o-mini",
    "azure:gpt-4o",
    "anthropic:claude-3-7-sonnet-latest",
    "gemini:gemini-1.5-pro",
    "llama:llama3-70b-8192"
  ],
  "request_timeout_seconds": 60.0,
  "error_message": "Benchmark failures encountered. Last error: BadRequestError: Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
}